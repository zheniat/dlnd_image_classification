{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 2:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 984, 1: 1007, 2: 1010, 3: 995, 4: 1010, 5: 988, 6: 1008, 7: 1026, 8: 987, 9: 985}\n",
      "First 20 Labels: [1, 6, 6, 8, 8, 3, 4, 6, 0, 6, 0, 3, 6, 6, 5, 4, 8, 3, 2, 6]\n",
      "\n",
      "Example of Image 2:\n",
      "Image - Min Value: 0 Max Value: 240\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 6 Name: frog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKEzRA0Bh/xOcxDplVQV0VQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fac59ddbf28>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 2\n",
    "sample_id = 2\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    return 0.1 + (x - x.min())*0.8/(x.max()-x.min())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "encoded_labels = dict()\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    return_labels = []\n",
    "    re_encode = False\n",
    "    for label in x:\n",
    "        if label not in encoded_labels:\n",
    "            re_encode = True\n",
    "            encoded_labels[label] = []\n",
    "\n",
    "    if re_encode:       \n",
    "        to_encode = []\n",
    "        for key, value in encoded_labels.items():\n",
    "            to_encode.append(key)\n",
    "        encoder = preprocessing.LabelBinarizer()\n",
    "        encoder.fit(to_encode)\n",
    "        new_labels = encoder.transform(to_encode)\n",
    "        for i in range(len(new_labels)):\n",
    "            encoded_labels[to_encode[i]] = new_labels[i]\n",
    "    \n",
    "    for label in x: \n",
    "        return_labels.append(encoded_labels[label])\n",
    "\n",
    "    # Change to float32, so it can be multiplied against the features in TensorFlow, which are float32\n",
    "    return np.asarray(return_labels, dtype=np.float32)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    shape = [None]\n",
    "    for s in image_shape:\n",
    "        shape.append(s)\n",
    "    \n",
    "    return tf.placeholder(tf.float32, shape=shape, name='x')\n",
    "    \n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=[None,n_classes],name='y')\n",
    "    \n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # Filter (weights and bias)\n",
    "    x_shape = x_tensor.get_shape().as_list() \n",
    "        \n",
    "    F_W = tf.Variable(tf.random_normal([conv_ksize[0],conv_ksize[1], x_shape[3] , conv_num_outputs], mean=0.0, stddev=0.05))\n",
    "    F_b = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, F_W, [1,conv_strides[0],conv_strides[1],1], 'SAME')\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, F_b)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    return tf.nn.max_pool(conv_layer, ksize=[1, pool_ksize[0], pool_ksize[1], 1], \\\n",
    "                          strides=[1, pool_strides[0], pool_strides[1], 1],\\\n",
    "                          padding='SAME')\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    size = shape[1]*shape[2]*shape[3]\n",
    "    return tf.reshape(x_tensor, [-1, size])\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    dims = x_tensor.get_shape().as_list()[1] #[0] is batch size\n",
    "    F_w = tf.Variable(tf.random_normal([dims, num_outputs], mean=0.0, stddev=0.05))\n",
    "    F_b = tf.Variable(tf.zeros(num_outputs))\n",
    "    v_res = tf.add(tf.matmul(x_tensor, F_w), F_b)\n",
    "    return tf.nn.relu(v_res)\n",
    "    \n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    O_w = tf.Variable(tf.random_normal([x_tensor.get_shape().as_list()[1], num_outputs], mean=0.0, stddev=0.05))\n",
    "    O_b = tf.Variable(tf.random_normal([num_outputs]))\n",
    "    return tf.add(tf.matmul(x_tensor, O_w),O_b)\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_outputs1 = 16\n",
    "    conv_ksize1 = (5, 5)\n",
    "    conv_strides1 = (1, 1)\n",
    "    pool_k1 = (2, 2)\n",
    "    pool_strides1 = (2, 2)\n",
    "    conv_net = conv2d_maxpool(x, conv_outputs1, conv_ksize1, conv_strides1, pool_k1, pool_strides1)\n",
    "    \n",
    "    conv_outputs2 = 64\n",
    "    conv_ksize2 = (2, 2)\n",
    "    conv_strides2 = (1, 1)\n",
    "    pool_k2 = (2, 2)\n",
    "    pool_strides2 = (2, 2)\n",
    "    conv_net = conv2d_maxpool(conv_net, conv_outputs2, conv_ksize2, conv_strides2, pool_k2, pool_strides2)\n",
    "    \n",
    "    conv_outputs3 = 128\n",
    "    conv_ksize3 = (2, 2)\n",
    "    conv_strides3 = (1, 1)\n",
    "    pool_k3 = (2, 2)\n",
    "    pool_strides3 = (1, 1)\n",
    "    conv_net = conv2d_maxpool(conv_net, conv_outputs2, conv_ksize2, conv_strides2, pool_k2, pool_strides2)\n",
    "    \n",
    "  \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    conv_net = flatten(conv_net)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    conv_net = fully_conn(conv_net, 100)\n",
    "    conv_net = tf.nn.dropout(conv_net, keep_prob)\n",
    "    \n",
    "    conv_net = fully_conn(conv_net, 50)\n",
    "    conv_net = tf.nn.dropout(conv_net, keep_prob)\n",
    "    \n",
    "    \n",
    "    conv_net = fully_conn(conv_net, 200)\n",
    "    conv_net = tf.nn.dropout(conv_net, keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    return output(conv_net, 10)\n",
    "    \n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "        # Run optimizer and get loss\n",
    "    optimizer = session.run(\n",
    "        [optimizer],\n",
    "        feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    valid_feed_dict = {x: valid_features, y: valid_labels, keep_prob: 1.0}\n",
    "    train_feed_dict = {x: feature_batch, y: label_batch, keep_prob: 1.0}\n",
    "\n",
    "    validation_accuracy = session.run(accuracy, feed_dict = valid_feed_dict)\n",
    "    validation_loss = session.run(cost, feed_dict=valid_feed_dict)  \n",
    "    train_loss = session.run(cost, feed_dict=train_feed_dict) \n",
    "    \n",
    "    print('Valid. accuracy at {}'.format(validation_accuracy), 'Valid. loss at {}'.format(validation_loss),\\\n",
    "          'Train loss at {}'.format(train_loss))\n",
    "\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs =35\n",
    "batch_size = 128\n",
    "keep_probability = 0.50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Valid. accuracy at 0.11179998517036438 Valid. loss at 2.311738967895508 Train loss at 2.302492618560791\n",
      "Epoch  2, CIFAR-10 Batch 1:  Valid. accuracy at 0.149399995803833 Valid. loss at 2.250971555709839 Train loss at 2.256598711013794\n",
      "Epoch  3, CIFAR-10 Batch 1:  Valid. accuracy at 0.1680000126361847 Valid. loss at 2.2020339965820312 Train loss at 2.20757794380188\n",
      "Epoch  4, CIFAR-10 Batch 1:  Valid. accuracy at 0.19760000705718994 Valid. loss at 2.1333234310150146 Train loss at 2.1454012393951416\n",
      "Epoch  5, CIFAR-10 Batch 1:  Valid. accuracy at 0.17379999160766602 Valid. loss at 2.1499667167663574 Train loss at 2.1969847679138184\n",
      "Epoch  6, CIFAR-10 Batch 1:  Valid. accuracy at 0.24739998579025269 Valid. loss at 2.0195109844207764 Train loss at 2.1246209144592285\n",
      "Epoch  7, CIFAR-10 Batch 1:  Valid. accuracy at 0.274399995803833 Valid. loss at 1.9435005187988281 Train loss at 2.1045193672180176\n",
      "Epoch  8, CIFAR-10 Batch 1:  Valid. accuracy at 0.32819998264312744 Valid. loss at 1.729384183883667 Train loss at 1.913377285003662\n",
      "Epoch  9, CIFAR-10 Batch 1:  Valid. accuracy at 0.34759998321533203 Valid. loss at 1.6956933736801147 Train loss at 1.8155771493911743\n",
      "Epoch 10, CIFAR-10 Batch 1:  Valid. accuracy at 0.3641999661922455 Valid. loss at 1.6580859422683716 Train loss at 1.7777801752090454\n",
      "Epoch 11, CIFAR-10 Batch 1:  Valid. accuracy at 0.36719995737075806 Valid. loss at 1.626766324043274 Train loss at 1.7460075616836548\n",
      "Epoch 12, CIFAR-10 Batch 1:  Valid. accuracy at 0.40139997005462646 Valid. loss at 1.6009567975997925 Train loss at 1.6696193218231201\n",
      "Epoch 13, CIFAR-10 Batch 1:  Valid. accuracy at 0.3967999815940857 Valid. loss at 1.596218228340149 Train loss at 1.6740281581878662\n",
      "Epoch 14, CIFAR-10 Batch 1:  Valid. accuracy at 0.42239996790885925 Valid. loss at 1.5355587005615234 Train loss at 1.5116859674453735\n",
      "Epoch 15, CIFAR-10 Batch 1:  Valid. accuracy at 0.4217999577522278 Valid. loss at 1.5536420345306396 Train loss at 1.5098259449005127\n",
      "Epoch 16, CIFAR-10 Batch 1:  Valid. accuracy at 0.40459996461868286 Valid. loss at 1.580758810043335 Train loss at 1.5357940196990967\n",
      "Epoch 17, CIFAR-10 Batch 1:  Valid. accuracy at 0.4183999300003052 Valid. loss at 1.568468689918518 Train loss at 1.4858195781707764\n",
      "Epoch 18, CIFAR-10 Batch 1:  Valid. accuracy at 0.44839996099472046 Valid. loss at 1.4635486602783203 Train loss at 1.3092303276062012\n",
      "Epoch 19, CIFAR-10 Batch 1:  Valid. accuracy at 0.44979995489120483 Valid. loss at 1.4548225402832031 Train loss at 1.2855890989303589\n",
      "Epoch 20, CIFAR-10 Batch 1:  Valid. accuracy at 0.4471999704837799 Valid. loss at 1.4958821535110474 Train loss at 1.2635554075241089\n",
      "Epoch 21, CIFAR-10 Batch 1:  Valid. accuracy at 0.45959994196891785 Valid. loss at 1.4549373388290405 Train loss at 1.2122761011123657\n",
      "Epoch 22, CIFAR-10 Batch 1:  Valid. accuracy at 0.4771999418735504 Valid. loss at 1.4355522394180298 Train loss at 1.1244218349456787\n",
      "Epoch 23, CIFAR-10 Batch 1:  Valid. accuracy at 0.46859994530677795 Valid. loss at 1.4635009765625 Train loss at 1.1484302282333374\n",
      "Epoch 24, CIFAR-10 Batch 1:  Valid. accuracy at 0.4843999445438385 Valid. loss at 1.4170045852661133 Train loss at 1.0572419166564941\n",
      "Epoch 25, CIFAR-10 Batch 1:  Valid. accuracy at 0.4909999370574951 Valid. loss at 1.428940773010254 Train loss at 1.1266579627990723\n",
      "Epoch 26, CIFAR-10 Batch 1:  Valid. accuracy at 0.5097999572753906 Valid. loss at 1.373201608657837 Train loss at 0.9406336545944214\n",
      "Epoch 27, CIFAR-10 Batch 1:  Valid. accuracy at 0.5071999430656433 Valid. loss at 1.358997106552124 Train loss at 0.9036773443222046\n",
      "Epoch 28, CIFAR-10 Batch 1:  Valid. accuracy at 0.500999927520752 Valid. loss at 1.4313135147094727 Train loss at 0.9223641753196716\n",
      "Epoch 29, CIFAR-10 Batch 1:  Valid. accuracy at 0.5147998929023743 Valid. loss at 1.376847743988037 Train loss at 0.9334762096405029\n",
      "Epoch 30, CIFAR-10 Batch 1:  Valid. accuracy at 0.5245999693870544 Valid. loss at 1.3392601013183594 Train loss at 0.8054869174957275\n",
      "Epoch 31, CIFAR-10 Batch 1:  Valid. accuracy at 0.51419997215271 Valid. loss at 1.3949302434921265 Train loss at 0.8174067139625549\n",
      "Epoch 32, CIFAR-10 Batch 1:  Valid. accuracy at 0.5369999408721924 Valid. loss at 1.3428336381912231 Train loss at 0.838935136795044\n",
      "Epoch 33, CIFAR-10 Batch 1:  Valid. accuracy at 0.5343999266624451 Valid. loss at 1.3485816717147827 Train loss at 0.7354485392570496\n",
      "Epoch 34, CIFAR-10 Batch 1:  Valid. accuracy at 0.5295999050140381 Valid. loss at 1.3563954830169678 Train loss at 0.7234783172607422\n",
      "Epoch 35, CIFAR-10 Batch 1:  Valid. accuracy at 0.5299999713897705 Valid. loss at 1.3539824485778809 Train loss at 0.7219157814979553\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Valid. accuracy at 0.10539999604225159 Valid. loss at 2.3419857025146484 Train loss at 2.3423006534576416\n",
      "Epoch  1, CIFAR-10 Batch 2:  Valid. accuracy at 0.10520000010728836 Valid. loss at 2.3121302127838135 Train loss at 2.3073081970214844\n",
      "Epoch  1, CIFAR-10 Batch 3:  Valid. accuracy at 0.125 Valid. loss at 2.2987701892852783 Train loss at 2.3011045455932617\n",
      "Epoch  1, CIFAR-10 Batch 4:  Valid. accuracy at 0.12960000336170197 Valid. loss at 2.2776753902435303 Train loss at 2.2562623023986816\n",
      "Epoch  1, CIFAR-10 Batch 5:  Valid. accuracy at 0.19599999487400055 Valid. loss at 2.0939905643463135 Train loss at 2.140636444091797\n",
      "Epoch  2, CIFAR-10 Batch 1:  Valid. accuracy at 0.20500001311302185 Valid. loss at 1.9752644300460815 Train loss at 2.1503093242645264\n",
      "Epoch  2, CIFAR-10 Batch 2:  Valid. accuracy at 0.21819999814033508 Valid. loss at 1.9103138446807861 Train loss at 2.0194997787475586\n",
      "Epoch  2, CIFAR-10 Batch 3:  Valid. accuracy at 0.2680000066757202 Valid. loss at 1.8699594736099243 Train loss at 1.7163326740264893\n",
      "Epoch  2, CIFAR-10 Batch 4:  Valid. accuracy at 0.30239996314048767 Valid. loss at 1.8195685148239136 Train loss at 1.7816033363342285\n",
      "Epoch  2, CIFAR-10 Batch 5:  Valid. accuracy at 0.3296000361442566 Valid. loss at 1.7581706047058105 Train loss at 1.725343942642212\n",
      "Epoch  3, CIFAR-10 Batch 1:  Valid. accuracy at 0.3527999520301819 Valid. loss at 1.7012395858764648 Train loss at 1.9053468704223633\n",
      "Epoch  3, CIFAR-10 Batch 2:  Valid. accuracy at 0.33799999952316284 Valid. loss at 1.6953227519989014 Train loss at 1.7939385175704956\n",
      "Epoch  3, CIFAR-10 Batch 3:  Valid. accuracy at 0.3483999967575073 Valid. loss at 1.6711262464523315 Train loss at 1.5380882024765015\n",
      "Epoch  3, CIFAR-10 Batch 4:  Valid. accuracy at 0.3564000129699707 Valid. loss at 1.6543471813201904 Train loss at 1.5651917457580566\n",
      "Epoch  3, CIFAR-10 Batch 5:  Valid. accuracy at 0.4059999883174896 Valid. loss at 1.572750210762024 Train loss at 1.6363645792007446\n",
      "Epoch  4, CIFAR-10 Batch 1:  Valid. accuracy at 0.40880000591278076 Valid. loss at 1.5516793727874756 Train loss at 1.6761884689331055\n",
      "Epoch  4, CIFAR-10 Batch 2:  Valid. accuracy at 0.4249999523162842 Valid. loss at 1.5273399353027344 Train loss at 1.6564009189605713\n",
      "Epoch  4, CIFAR-10 Batch 3:  Valid. accuracy at 0.3991999924182892 Valid. loss at 1.5781686305999756 Train loss at 1.4026705026626587\n",
      "Epoch  4, CIFAR-10 Batch 4:  Valid. accuracy at 0.43219998478889465 Valid. loss at 1.501603126525879 Train loss at 1.4455418586730957\n",
      "Epoch  4, CIFAR-10 Batch 5:  Valid. accuracy at 0.4357999861240387 Valid. loss at 1.4918112754821777 Train loss at 1.550041913986206\n",
      "Epoch  5, CIFAR-10 Batch 1:  Valid. accuracy at 0.42800000309944153 Valid. loss at 1.5018277168273926 Train loss at 1.6289259195327759\n",
      "Epoch  5, CIFAR-10 Batch 2:  Valid. accuracy at 0.46519994735717773 Valid. loss at 1.4335403442382812 Train loss at 1.4557931423187256\n",
      "Epoch  5, CIFAR-10 Batch 3:  Valid. accuracy at 0.4553999602794647 Valid. loss at 1.4850077629089355 Train loss at 1.3220832347869873\n",
      "Epoch  5, CIFAR-10 Batch 4:  Valid. accuracy at 0.4575999975204468 Valid. loss at 1.4395561218261719 Train loss at 1.3833876848220825\n",
      "Epoch  5, CIFAR-10 Batch 5:  Valid. accuracy at 0.4505999684333801 Valid. loss at 1.4858890771865845 Train loss at 1.5276705026626587\n",
      "Epoch  6, CIFAR-10 Batch 1:  Valid. accuracy at 0.49779993295669556 Valid. loss at 1.3691977262496948 Train loss at 1.4285671710968018\n",
      "Epoch  6, CIFAR-10 Batch 2:  Valid. accuracy at 0.4971999526023865 Valid. loss at 1.359428882598877 Train loss at 1.3123427629470825\n",
      "Epoch  6, CIFAR-10 Batch 3:  Valid. accuracy at 0.480199933052063 Valid. loss at 1.3916075229644775 Train loss at 1.262539029121399\n",
      "Epoch  6, CIFAR-10 Batch 4:  Valid. accuracy at 0.4833999276161194 Valid. loss at 1.386596918106079 Train loss at 1.3484543561935425\n",
      "Epoch  6, CIFAR-10 Batch 5:  Valid. accuracy at 0.48499998450279236 Valid. loss at 1.3972887992858887 Train loss at 1.4132819175720215\n",
      "Epoch  7, CIFAR-10 Batch 1:  Valid. accuracy at 0.5031999349594116 Valid. loss at 1.3444472551345825 Train loss at 1.3473631143569946\n",
      "Epoch  7, CIFAR-10 Batch 2:  Valid. accuracy at 0.5007999539375305 Valid. loss at 1.3527363538742065 Train loss at 1.2588863372802734\n",
      "Epoch  7, CIFAR-10 Batch 3:  Valid. accuracy at 0.514799952507019 Valid. loss at 1.327444076538086 Train loss at 1.1698744297027588\n",
      "Epoch  7, CIFAR-10 Batch 4:  Valid. accuracy at 0.5149999856948853 Valid. loss at 1.3010376691818237 Train loss at 1.2556898593902588\n",
      "Epoch  7, CIFAR-10 Batch 5:  Valid. accuracy at 0.5003999471664429 Valid. loss at 1.3309389352798462 Train loss at 1.2594294548034668\n",
      "Epoch  8, CIFAR-10 Batch 1:  Valid. accuracy at 0.5251999497413635 Valid. loss at 1.3169240951538086 Train loss at 1.3409197330474854\n",
      "Epoch  8, CIFAR-10 Batch 2:  Valid. accuracy at 0.533799946308136 Valid. loss at 1.2702974081039429 Train loss at 1.1920593976974487\n",
      "Epoch  8, CIFAR-10 Batch 3:  Valid. accuracy at 0.5415999293327332 Valid. loss at 1.2671825885772705 Train loss at 1.1251944303512573\n",
      "Epoch  8, CIFAR-10 Batch 4:  Valid. accuracy at 0.5295999646186829 Valid. loss at 1.2925920486450195 Train loss at 1.194016695022583\n",
      "Epoch  8, CIFAR-10 Batch 5:  Valid. accuracy at 0.556399941444397 Valid. loss at 1.2246267795562744 Train loss at 1.1621475219726562\n",
      "Epoch  9, CIFAR-10 Batch 1:  Valid. accuracy at 0.5577999353408813 Valid. loss at 1.2526531219482422 Train loss at 1.1673041582107544\n",
      "Epoch  9, CIFAR-10 Batch 2:  Valid. accuracy at 0.5547999143600464 Valid. loss at 1.202301263809204 Train loss at 1.1442586183547974\n",
      "Epoch  9, CIFAR-10 Batch 3:  Valid. accuracy at 0.5431999564170837 Valid. loss at 1.247152328491211 Train loss at 1.0622751712799072\n",
      "Epoch  9, CIFAR-10 Batch 4:  Valid. accuracy at 0.5501999855041504 Valid. loss at 1.2663017511367798 Train loss at 1.1070897579193115\n",
      "Epoch  9, CIFAR-10 Batch 5:  Valid. accuracy at 0.562999963760376 Valid. loss at 1.2114267349243164 Train loss at 1.0774738788604736\n",
      "Epoch 10, CIFAR-10 Batch 1:  Valid. accuracy at 0.5619999170303345 Valid. loss at 1.2253708839416504 Train loss at 1.0975717306137085\n",
      "Epoch 10, CIFAR-10 Batch 2:  Valid. accuracy at 0.5751999616622925 Valid. loss at 1.1896792650222778 Train loss at 1.0409904718399048\n",
      "Epoch 10, CIFAR-10 Batch 3:  Valid. accuracy at 0.5701999664306641 Valid. loss at 1.1870698928833008 Train loss at 1.0400060415267944\n",
      "Epoch 10, CIFAR-10 Batch 4:  Valid. accuracy at 0.5641999244689941 Valid. loss at 1.2433215379714966 Train loss at 1.1018009185791016\n",
      "Epoch 10, CIFAR-10 Batch 5:  Valid. accuracy at 0.5661998987197876 Valid. loss at 1.2048405408859253 Train loss at 1.0786901712417603\n",
      "Epoch 11, CIFAR-10 Batch 1:  Valid. accuracy at 0.5717998743057251 Valid. loss at 1.199059247970581 Train loss at 1.038106083869934\n",
      "Epoch 11, CIFAR-10 Batch 2:  Valid. accuracy at 0.5869998931884766 Valid. loss at 1.165466070175171 Train loss at 1.0782204866409302\n",
      "Epoch 11, CIFAR-10 Batch 3:  Valid. accuracy at 0.5753999352455139 Valid. loss at 1.1880815029144287 Train loss at 0.8959039449691772\n",
      "Epoch 11, CIFAR-10 Batch 4:  Valid. accuracy at 0.5875999331474304 Valid. loss at 1.158490538597107 Train loss at 1.1036580801010132\n",
      "Epoch 11, CIFAR-10 Batch 5:  Valid. accuracy at 0.5865998864173889 Valid. loss at 1.183180570602417 Train loss at 0.9375760555267334\n",
      "Epoch 12, CIFAR-10 Batch 1:  Valid. accuracy at 0.5951999425888062 Valid. loss at 1.1575549840927124 Train loss at 0.9395129084587097\n",
      "Epoch 12, CIFAR-10 Batch 2:  Valid. accuracy at 0.6045998930931091 Valid. loss at 1.1350603103637695 Train loss at 0.9573958516120911\n",
      "Epoch 12, CIFAR-10 Batch 3:  Valid. accuracy at 0.6041998863220215 Valid. loss at 1.1357163190841675 Train loss at 0.8959494233131409\n",
      "Epoch 12, CIFAR-10 Batch 4:  Valid. accuracy at 0.5989999175071716 Valid. loss at 1.1503806114196777 Train loss at 1.0365113019943237\n",
      "Epoch 12, CIFAR-10 Batch 5:  Valid. accuracy at 0.5871999263763428 Valid. loss at 1.1525644063949585 Train loss at 0.9791536331176758\n",
      "Epoch 13, CIFAR-10 Batch 1:  Valid. accuracy at 0.611799955368042 Valid. loss at 1.120986819267273 Train loss at 0.9558937549591064\n",
      "Epoch 13, CIFAR-10 Batch 2:  Valid. accuracy at 0.5937999486923218 Valid. loss at 1.1339566707611084 Train loss at 0.8628425598144531\n",
      "Epoch 13, CIFAR-10 Batch 3:  Valid. accuracy at 0.6113998889923096 Valid. loss at 1.1224007606506348 Train loss at 0.8879411220550537\n",
      "Epoch 13, CIFAR-10 Batch 4:  Valid. accuracy at 0.6119999289512634 Valid. loss at 1.1166990995407104 Train loss at 1.0113811492919922\n",
      "Epoch 13, CIFAR-10 Batch 5:  Valid. accuracy at 0.6077999472618103 Valid. loss at 1.1134904623031616 Train loss at 0.8398164510726929\n",
      "Epoch 14, CIFAR-10 Batch 1:  Valid. accuracy at 0.6161999106407166 Valid. loss at 1.0953115224838257 Train loss at 0.9075121283531189\n",
      "Epoch 14, CIFAR-10 Batch 2:  Valid. accuracy at 0.6151999235153198 Valid. loss at 1.1130571365356445 Train loss at 0.8986022472381592\n",
      "Epoch 14, CIFAR-10 Batch 3:  Valid. accuracy at 0.6061999201774597 Valid. loss at 1.1087568998336792 Train loss at 0.7676104307174683\n",
      "Epoch 14, CIFAR-10 Batch 4:  Valid. accuracy at 0.5991998910903931 Valid. loss at 1.1425694227218628 Train loss at 1.0293574333190918\n",
      "Epoch 14, CIFAR-10 Batch 5:  Valid. accuracy at 0.6149998903274536 Valid. loss at 1.1098295450210571 Train loss at 0.9383862614631653\n",
      "Epoch 15, CIFAR-10 Batch 1:  Valid. accuracy at 0.6111999154090881 Valid. loss at 1.1168320178985596 Train loss at 0.9800724983215332\n",
      "Epoch 15, CIFAR-10 Batch 2:  Valid. accuracy at 0.6257998943328857 Valid. loss at 1.091341495513916 Train loss at 0.845909595489502\n",
      "Epoch 15, CIFAR-10 Batch 3:  Valid. accuracy at 0.6131998896598816 Valid. loss at 1.101689100265503 Train loss at 0.7230314612388611\n",
      "Epoch 15, CIFAR-10 Batch 4:  Valid. accuracy at 0.6141999363899231 Valid. loss at 1.114612102508545 Train loss at 1.0487879514694214\n",
      "Epoch 15, CIFAR-10 Batch 5:  Valid. accuracy at 0.6191998720169067 Valid. loss at 1.101559042930603 Train loss at 0.7688056230545044\n",
      "Epoch 16, CIFAR-10 Batch 1:  Valid. accuracy at 0.6117998957633972 Valid. loss at 1.1152523756027222 Train loss at 0.8446433544158936\n",
      "Epoch 16, CIFAR-10 Batch 2:  Valid. accuracy at 0.6339999437332153 Valid. loss at 1.0711588859558105 Train loss at 0.7724581956863403\n",
      "Epoch 16, CIFAR-10 Batch 3:  Valid. accuracy at 0.6161999702453613 Valid. loss at 1.1044399738311768 Train loss at 0.698967695236206\n",
      "Epoch 16, CIFAR-10 Batch 4:  Valid. accuracy at 0.627799928188324 Valid. loss at 1.0902159214019775 Train loss at 0.9715234637260437\n",
      "Epoch 16, CIFAR-10 Batch 5:  Valid. accuracy at 0.6059999465942383 Valid. loss at 1.1300678253173828 Train loss at 0.9106886386871338\n",
      "Epoch 17, CIFAR-10 Batch 1:  Valid. accuracy at 0.6195999383926392 Valid. loss at 1.0913279056549072 Train loss at 0.8724808096885681\n",
      "Epoch 17, CIFAR-10 Batch 2:  Valid. accuracy at 0.6423999071121216 Valid. loss at 1.0672721862792969 Train loss at 0.7950223684310913\n",
      "Epoch 17, CIFAR-10 Batch 3:  Valid. accuracy at 0.6279999017715454 Valid. loss at 1.0674034357070923 Train loss at 0.6350835561752319\n",
      "Epoch 17, CIFAR-10 Batch 4:  Valid. accuracy at 0.6369999051094055 Valid. loss at 1.067854881286621 Train loss at 0.958046555519104\n",
      "Epoch 17, CIFAR-10 Batch 5:  Valid. accuracy at 0.6283998489379883 Valid. loss at 1.0846011638641357 Train loss at 0.7997688055038452\n",
      "Epoch 18, CIFAR-10 Batch 1:  Valid. accuracy at 0.643799901008606 Valid. loss at 1.0508570671081543 Train loss at 0.8047927618026733\n",
      "Epoch 18, CIFAR-10 Batch 2:  Valid. accuracy at 0.640799880027771 Valid. loss at 1.0614081621170044 Train loss at 0.7654407024383545\n",
      "Epoch 18, CIFAR-10 Batch 3:  Valid. accuracy at 0.6339998841285706 Valid. loss at 1.0657439231872559 Train loss at 0.7038744688034058\n",
      "Epoch 18, CIFAR-10 Batch 4:  Valid. accuracy at 0.6357998847961426 Valid. loss at 1.08444344997406 Train loss at 0.9218100905418396\n",
      "Epoch 18, CIFAR-10 Batch 5:  Valid. accuracy at 0.6353998780250549 Valid. loss at 1.0515549182891846 Train loss at 0.8037287592887878\n",
      "Epoch 19, CIFAR-10 Batch 1:  Valid. accuracy at 0.6395999193191528 Valid. loss at 1.05350661277771 Train loss at 0.7960045337677002\n",
      "Epoch 19, CIFAR-10 Batch 2:  Valid. accuracy at 0.639799952507019 Valid. loss at 1.0588774681091309 Train loss at 0.8119221925735474\n",
      "Epoch 19, CIFAR-10 Batch 3:  Valid. accuracy at 0.6343998908996582 Valid. loss at 1.0666390657424927 Train loss at 0.6720953583717346\n",
      "Epoch 19, CIFAR-10 Batch 4:  Valid. accuracy at 0.6193999648094177 Valid. loss at 1.10679292678833 Train loss at 0.9389376640319824\n",
      "Epoch 19, CIFAR-10 Batch 5:  Valid. accuracy at 0.6441998481750488 Valid. loss at 1.0433439016342163 Train loss at 0.7650970816612244\n",
      "Epoch 20, CIFAR-10 Batch 1:  Valid. accuracy at 0.6359999179840088 Valid. loss at 1.064112663269043 Train loss at 0.7273364067077637\n",
      "Epoch 20, CIFAR-10 Batch 2:  Valid. accuracy at 0.6513998508453369 Valid. loss at 1.0338729619979858 Train loss at 0.7424987554550171\n",
      "Epoch 20, CIFAR-10 Batch 3:  Valid. accuracy at 0.6337999105453491 Valid. loss at 1.070310115814209 Train loss at 0.6345147490501404\n",
      "Epoch 20, CIFAR-10 Batch 4:  Valid. accuracy at 0.6319998502731323 Valid. loss at 1.0508201122283936 Train loss at 0.8867285251617432\n",
      "Epoch 20, CIFAR-10 Batch 5:  Valid. accuracy at 0.6465998888015747 Valid. loss at 1.041682243347168 Train loss at 0.7031687498092651\n",
      "Epoch 21, CIFAR-10 Batch 1:  Valid. accuracy at 0.6369999051094055 Valid. loss at 1.0473449230194092 Train loss at 0.7525989413261414\n",
      "Epoch 21, CIFAR-10 Batch 2:  Valid. accuracy at 0.6363999247550964 Valid. loss at 1.0546290874481201 Train loss at 0.8092879056930542\n",
      "Epoch 21, CIFAR-10 Batch 3:  Valid. accuracy at 0.6415998935699463 Valid. loss at 1.0490124225616455 Train loss at 0.6050260066986084\n",
      "Epoch 21, CIFAR-10 Batch 4:  Valid. accuracy at 0.639799952507019 Valid. loss at 1.0534917116165161 Train loss at 0.8645393252372742\n",
      "Epoch 21, CIFAR-10 Batch 5:  Valid. accuracy at 0.6453998684883118 Valid. loss at 1.0316598415374756 Train loss at 0.7394051551818848\n",
      "Epoch 22, CIFAR-10 Batch 1:  Valid. accuracy at 0.6521998643875122 Valid. loss at 1.0445504188537598 Train loss at 0.7269725799560547\n",
      "Epoch 22, CIFAR-10 Batch 2:  Valid. accuracy at 0.6351999044418335 Valid. loss at 1.076273798942566 Train loss at 0.7786363363265991\n",
      "Epoch 22, CIFAR-10 Batch 3:  Valid. accuracy at 0.6411998867988586 Valid. loss at 1.0508997440338135 Train loss at 0.6310153603553772\n",
      "Epoch 22, CIFAR-10 Batch 4:  Valid. accuracy at 0.6365998983383179 Valid. loss at 1.0683842897415161 Train loss at 0.8010188341140747\n",
      "Epoch 22, CIFAR-10 Batch 5:  Valid. accuracy at 0.6487998962402344 Valid. loss at 1.047025442123413 Train loss at 0.7327261567115784\n",
      "Epoch 23, CIFAR-10 Batch 1:  Valid. accuracy at 0.6469998955726624 Valid. loss at 1.0204792022705078 Train loss at 0.7020302414894104\n",
      "Epoch 23, CIFAR-10 Batch 2:  Valid. accuracy at 0.6521998643875122 Valid. loss at 1.0159077644348145 Train loss at 0.7388055920600891\n",
      "Epoch 23, CIFAR-10 Batch 3:  Valid. accuracy at 0.6357998847961426 Valid. loss at 1.0603300333023071 Train loss at 0.6367802619934082\n",
      "Epoch 23, CIFAR-10 Batch 4:  Valid. accuracy at 0.6493998765945435 Valid. loss at 1.0152755975723267 Train loss at 0.7488322257995605\n",
      "Epoch 23, CIFAR-10 Batch 5:  Valid. accuracy at 0.6455998420715332 Valid. loss at 1.0299729108810425 Train loss at 0.6951697468757629\n",
      "Epoch 24, CIFAR-10 Batch 1:  Valid. accuracy at 0.6483998894691467 Valid. loss at 1.0173271894454956 Train loss at 0.7327816486358643\n",
      "Epoch 24, CIFAR-10 Batch 2:  Valid. accuracy at 0.6483998894691467 Valid. loss at 1.0430009365081787 Train loss at 0.7585693001747131\n",
      "Epoch 24, CIFAR-10 Batch 3:  Valid. accuracy at 0.644399881362915 Valid. loss at 1.0383825302124023 Train loss at 0.5557491183280945\n",
      "Epoch 24, CIFAR-10 Batch 4:  Valid. accuracy at 0.6453998684883118 Valid. loss at 1.033508062362671 Train loss at 0.7558104991912842\n",
      "Epoch 24, CIFAR-10 Batch 5:  Valid. accuracy at 0.6573998928070068 Valid. loss at 1.0239309072494507 Train loss at 0.6564129590988159\n",
      "Epoch 25, CIFAR-10 Batch 1:  Valid. accuracy at 0.6589999198913574 Valid. loss at 1.013932228088379 Train loss at 0.7173477411270142\n",
      "Epoch 25, CIFAR-10 Batch 2:  Valid. accuracy at 0.6551998853683472 Valid. loss at 1.0205813646316528 Train loss at 0.722067654132843\n",
      "Epoch 25, CIFAR-10 Batch 3:  Valid. accuracy at 0.6529998779296875 Valid. loss at 1.0151293277740479 Train loss at 0.5329539179801941\n",
      "Epoch 25, CIFAR-10 Batch 4:  Valid. accuracy at 0.6445999145507812 Valid. loss at 1.0368608236312866 Train loss at 0.7127842903137207\n",
      "Epoch 25, CIFAR-10 Batch 5:  Valid. accuracy at 0.6529998779296875 Valid. loss at 1.0223784446716309 Train loss at 0.5725816488265991\n",
      "Epoch 26, CIFAR-10 Batch 1:  Valid. accuracy at 0.6543998718261719 Valid. loss at 1.04231858253479 Train loss at 0.713192880153656\n",
      "Epoch 26, CIFAR-10 Batch 2:  Valid. accuracy at 0.6419999003410339 Valid. loss at 1.0640802383422852 Train loss at 0.7292722463607788\n",
      "Epoch 26, CIFAR-10 Batch 3:  Valid. accuracy at 0.6237998604774475 Valid. loss at 1.092002511024475 Train loss at 0.5759897828102112\n",
      "Epoch 26, CIFAR-10 Batch 4:  Valid. accuracy at 0.6533998847007751 Valid. loss at 1.0306644439697266 Train loss at 0.7842117547988892\n",
      "Epoch 26, CIFAR-10 Batch 5:  Valid. accuracy at 0.6573998928070068 Valid. loss at 1.0145976543426514 Train loss at 0.581641435623169\n",
      "Epoch 27, CIFAR-10 Batch 1:  Valid. accuracy at 0.6497998833656311 Valid. loss at 1.0182278156280518 Train loss at 0.6738523840904236\n",
      "Epoch 27, CIFAR-10 Batch 2:  Valid. accuracy at 0.6419999003410339 Valid. loss at 1.0541075468063354 Train loss at 0.7687801122665405\n",
      "Epoch 27, CIFAR-10 Batch 3:  Valid. accuracy at 0.6575998663902283 Valid. loss at 1.008553385734558 Train loss at 0.47493985295295715\n",
      "Epoch 27, CIFAR-10 Batch 4:  Valid. accuracy at 0.6505998373031616 Valid. loss at 1.023149013519287 Train loss at 0.6906867027282715\n",
      "Epoch 27, CIFAR-10 Batch 5:  Valid. accuracy at 0.6589998006820679 Valid. loss at 1.0144411325454712 Train loss at 0.5802689790725708\n",
      "Epoch 28, CIFAR-10 Batch 1:  Valid. accuracy at 0.6609998941421509 Valid. loss at 1.0104104280471802 Train loss at 0.6451584696769714\n",
      "Epoch 28, CIFAR-10 Batch 2:  Valid. accuracy at 0.6623998880386353 Valid. loss at 1.0168203115463257 Train loss at 0.6397354006767273\n",
      "Epoch 28, CIFAR-10 Batch 3:  Valid. accuracy at 0.6395999193191528 Valid. loss at 1.0772055387496948 Train loss at 0.5221430063247681\n",
      "Epoch 28, CIFAR-10 Batch 4:  Valid. accuracy at 0.6579998731613159 Valid. loss at 1.0129661560058594 Train loss at 0.6922203898429871\n",
      "Epoch 28, CIFAR-10 Batch 5:  Valid. accuracy at 0.663399875164032 Valid. loss at 0.9996110796928406 Train loss at 0.5447068214416504\n",
      "Epoch 29, CIFAR-10 Batch 1:  Valid. accuracy at 0.6547998189926147 Valid. loss at 1.0156335830688477 Train loss at 0.7121446132659912\n",
      "Epoch 29, CIFAR-10 Batch 2:  Valid. accuracy at 0.6663998365402222 Valid. loss at 0.9950608015060425 Train loss at 0.6353529691696167\n",
      "Epoch 29, CIFAR-10 Batch 3:  Valid. accuracy at 0.6495999097824097 Valid. loss at 1.0416456460952759 Train loss at 0.4761914312839508\n",
      "Epoch 29, CIFAR-10 Batch 4:  Valid. accuracy at 0.6665998697280884 Valid. loss at 0.9867342114448547 Train loss at 0.6708423495292664\n",
      "Epoch 29, CIFAR-10 Batch 5:  Valid. accuracy at 0.6617999076843262 Valid. loss at 1.000119924545288 Train loss at 0.614623486995697\n",
      "Epoch 30, CIFAR-10 Batch 1:  Valid. accuracy at 0.6575998663902283 Valid. loss at 1.0250613689422607 Train loss at 0.6351027488708496\n",
      "Epoch 30, CIFAR-10 Batch 2:  Valid. accuracy at 0.6685998439788818 Valid. loss at 0.9941293001174927 Train loss at 0.5902911424636841\n",
      "Epoch 30, CIFAR-10 Batch 3:  Valid. accuracy at 0.6445999145507812 Valid. loss at 1.0409431457519531 Train loss at 0.48369312286376953\n",
      "Epoch 30, CIFAR-10 Batch 4:  Valid. accuracy at 0.6649999022483826 Valid. loss at 0.9949573278427124 Train loss at 0.6137688159942627\n",
      "Epoch 30, CIFAR-10 Batch 5:  Valid. accuracy at 0.6703999042510986 Valid. loss at 0.989664614200592 Train loss at 0.532505989074707\n",
      "Epoch 31, CIFAR-10 Batch 1:  Valid. accuracy at 0.6631999015808105 Valid. loss at 1.0016381740570068 Train loss at 0.638486385345459\n",
      "Epoch 31, CIFAR-10 Batch 2:  Valid. accuracy at 0.6643998622894287 Valid. loss at 1.0103256702423096 Train loss at 0.6011556386947632\n",
      "Epoch 31, CIFAR-10 Batch 3:  Valid. accuracy at 0.6459999084472656 Valid. loss at 1.042493462562561 Train loss at 0.47964751720428467\n",
      "Epoch 31, CIFAR-10 Batch 4:  Valid. accuracy at 0.6505998969078064 Valid. loss at 1.0253794193267822 Train loss at 0.6544555425643921\n",
      "Epoch 31, CIFAR-10 Batch 5:  Valid. accuracy at 0.6625998616218567 Valid. loss at 0.9936391711235046 Train loss at 0.556784987449646\n",
      "Epoch 32, CIFAR-10 Batch 1:  Valid. accuracy at 0.6531999707221985 Valid. loss at 1.025369644165039 Train loss at 0.6518594026565552\n",
      "Epoch 32, CIFAR-10 Batch 2:  Valid. accuracy at 0.660399854183197 Valid. loss at 1.0169974565505981 Train loss at 0.5472808480262756\n",
      "Epoch 32, CIFAR-10 Batch 3:  Valid. accuracy at 0.6585999131202698 Valid. loss at 1.0190218687057495 Train loss at 0.41783177852630615\n",
      "Epoch 32, CIFAR-10 Batch 4:  Valid. accuracy at 0.6587998867034912 Valid. loss at 1.0100153684616089 Train loss at 0.6514776349067688\n",
      "Epoch 32, CIFAR-10 Batch 5:  Valid. accuracy at 0.6607998609542847 Valid. loss at 1.0155352354049683 Train loss at 0.6019020080566406\n",
      "Epoch 33, CIFAR-10 Batch 1:  Valid. accuracy at 0.6743998527526855 Valid. loss at 0.9884433150291443 Train loss at 0.6202467679977417\n",
      "Epoch 33, CIFAR-10 Batch 2:  Valid. accuracy at 0.6553998589515686 Valid. loss at 1.0312374830245972 Train loss at 0.5393986105918884\n",
      "Epoch 33, CIFAR-10 Batch 3:  Valid. accuracy at 0.66239994764328 Valid. loss at 1.0058456659317017 Train loss at 0.40203365683555603\n",
      "Epoch 33, CIFAR-10 Batch 4:  Valid. accuracy at 0.6643999218940735 Valid. loss at 0.9943047761917114 Train loss at 0.6445095539093018\n",
      "Epoch 33, CIFAR-10 Batch 5:  Valid. accuracy at 0.668199896812439 Valid. loss at 0.9891852736473083 Train loss at 0.507538914680481\n",
      "Epoch 34, CIFAR-10 Batch 1:  Valid. accuracy at 0.6625998616218567 Valid. loss at 1.0200730562210083 Train loss at 0.5998972058296204\n",
      "Epoch 34, CIFAR-10 Batch 2:  Valid. accuracy at 0.6691998243331909 Valid. loss at 0.99592524766922 Train loss at 0.6267027258872986\n",
      "Epoch 34, CIFAR-10 Batch 3:  Valid. accuracy at 0.6529998779296875 Valid. loss at 1.0027482509613037 Train loss at 0.46906155347824097\n",
      "Epoch 34, CIFAR-10 Batch 4:  Valid. accuracy at 0.6565999388694763 Valid. loss at 1.0240596532821655 Train loss at 0.6084595322608948\n",
      "Epoch 34, CIFAR-10 Batch 5:  Valid. accuracy at 0.6619999408721924 Valid. loss at 1.0103511810302734 Train loss at 0.5069259405136108\n",
      "Epoch 35, CIFAR-10 Batch 1:  Valid. accuracy at 0.6745998859405518 Valid. loss at 0.9852613210678101 Train loss at 0.6044130325317383\n",
      "Epoch 35, CIFAR-10 Batch 2:  Valid. accuracy at 0.6533999443054199 Valid. loss at 1.0297904014587402 Train loss at 0.6202390193939209\n",
      "Epoch 35, CIFAR-10 Batch 3:  Valid. accuracy at 0.6591998934745789 Valid. loss at 1.0424214601516724 Train loss at 0.43434327840805054\n",
      "Epoch 35, CIFAR-10 Batch 4:  Valid. accuracy at 0.6677999496459961 Valid. loss at 0.9926193356513977 Train loss at 0.5692280530929565\n",
      "Epoch 35, CIFAR-10 Batch 5:  Valid. accuracy at 0.6643998622894287 Valid. loss at 1.0060232877731323 Train loss at 0.5099027156829834\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.6668314873417721\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxbGIiIiIiPt/WQ9Ery2sCV8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f463087d4e0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
